{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPso416ouyoy4IzEPDCgcJc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"K5RGhFFKgw6M"},"outputs":[],"source":["!pip install \"shapely<2.0.0\"\n","!pip install google-cloud-aiplatform>=1.26.0"]},{"cell_type":"code","source":["from google.colab import auth as google_auth\n","google_auth.authenticate_user()\n","\n","import vertexai\n","from vertexai.language_models import TextGenerationModel\n","\n","vertexai.init(project=\"lively-ace-392206\", location=\"us-central1\")\n","parameters = {\n","    \"temperature\": 0.2,\n","    \"max_output_tokens\": 800,\n","    \"top_p\": 0.8,\n","    \"top_k\": 40\n","}\n","model = TextGenerationModel.from_pretrained(\"text-bison@001\")"],"metadata":{"id":"kpnq1hIAg8ft","executionInfo":{"status":"ok","timestamp":1688813309912,"user_tz":-120,"elapsed":2106,"user":{"displayName":"Marcel Tracz","userId":"16306824067966008324"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["delimiter = \"####\"\n","prompt1 = f\"\"\"\n","You will be given just one page of a scientific paper with text only, you won't be able to see the figures.\n","\n","Set of rules you must follow:\n","Do not summarize the page or make a recommendation for acceptance or rejection. This is not your job.\n","Do not build on the content with your personal knowledge, you should only review the content of the page.\n","Do not include criteria that are not relevant to the page given to you.\n","Review only the page given to you, do not point things out that are not on the page as they may be addressed in other pages.\n","\n","Remember to follow the set of rules.\n","Each query will be delimited with four hashtags, i.e. {delimiter}.\n","\n","Follow these steps to conduct a review of a page from a scientific paper:\n","Step 1:{delimiter} Check if the page has a coherent structure and logical arrangement of content.\n","Step 2:{delimiter} Verify if the introduction, results, discussion, and conclusions are appropriately presented.\n","Step 3:{delimiter} Take note of any errors, gaps, or inconsistencies in the article, both in terms of substantive content and technical aspects.\n","Step 4:{delimiter} Verify if the conclusions align with the gathered data.\n","Step 5:{delimiter} Check if the author have correctly cited and referred to relevant sources.\n","Step 6:{delimiter} Check if the paper is written in a formal, objective, and precise language. The use of colloquialisms, slang expressions, and informal phrases is prohibited.\n","Step 7:{delimiter} Prepare a formal review of the page.\n","\n","Use the following format (2 sentences for each step):\n","Step 1:\n","{delimiter} <step 1 reasoning>\n","Step 2:\n","{delimiter} <step 2 reasoning>\n","Step 3:\n","{delimiter} <step 3 reasoning>\n","Step 4:\n","{delimiter} <step 4 reasoning>\n","Step 5:\n","{delimiter} <step 5 reasoning>\n","Step 6:\n","{delimiter} <step 6 reasoning>\n","Step 7:\n","{delimiter} <step 7 reasoning>\n","\n","Remember to use the right format and give some details.\n","Make sure to include {delimiter} to separate every step.\n","Remember that you are reviewing only one page of the paper.\n","Use the delimiters.\n","\"\"\""],"metadata":{"id":"Khm-E6C0hDJn","executionInfo":{"status":"ok","timestamp":1688823213205,"user_tz":-120,"elapsed":417,"user":{"displayName":"Marcel Tracz","userId":"16306824067966008324"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["text1 = \"\"\"\n","A Style-Based Generator Architecture for Generative Adversarial Networks\n","Tero Karras\n","NVIDIA\n","tkarras@nvidia.com\n","Samuli Laine\n","NVIDIA\n","slaine@nvidia.com\n","Timo Aila\n","NVIDIA\n","taila@nvidia.com\n","\n","Abstract\n","We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature.\n","The new architecture leads to an automatically learned, unsupervised separation of high-level attributes\n","(e.g., pose and identity when trained on human faces) and stochastic variation in the generated images\n","(e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis.\n","The new generator improves the state-of-the-art in terms of traditional distribution quality metrics,\n","leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation.\n","To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture.\n","Finally, we introduce a new, highly varied and high-quality dataset of human faces.\n","\n","Introduction\n","The resolution and quality of images produced by generative methods — especially generative adversarial networks\n","(GAN) [22] — have seen rapid improvement recently [30, 45, 5]. Yet the generators continue to operate as black boxes, and despite recent efforts [3],\n","the understanding of various aspects of the image synthesis process, e.g., the origin of stochastic features, is still lacking.\n","The properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations [13, 52, 37]\n","provide no quantitative way to compare different generators against each other.\n","Motivated by style transfer literature [27], we re-design the generator architecture in a way that exposes novel ways to control the image synthesis process.\n","Our generator starts from a learned constant input and adjusts the “style” of the image at each convolution layer based on the latent code,\n","therefore directly controlling the strength of image features at different scales. Combined with noise injected directly into the network,\n","this architectural change leads to automatic, unsupervised separation of high-level attributes\n","(e.g., pose, identity) from stochastic variation (e.g., freckles, hair) in the generated images,\n","and enables intuitive scale-specific mixing and interpolation operations.\n","We do not modify the discriminator or the loss function in any way, and our work is thus orthogonal to the ongoing discussion about GAN loss functions,\n","regularization, and hyperparameters [24, 45, 5, 40, 44, 36]. Our generator embeds the input latent code into an intermediate latent space,\n","which has a profound effect on how the factors of variation are represented in the network.\n","The input latent space must follow the probability density of the training data, and we argue that this leads to some degree of unavoidable entanglement.\n","Our intermediate latent space is free from that restriction and is therefore allowed to be disentangled.\n","As previous methods for estimating the degree of latent space disentanglement are not directly applicable in our case,\n","we propose two new automated metrics — perceptual path length and linear separability — for quantifying these aspects of the generator.\n","Using these metrics, we show that compared to a traditional generator architecture,\n","our generator admits a more linear, less entangled representation of different factors of variation.\n","Finally, we present a new dataset of human faces (Flickr-Faces-HQ, FFHQ) that offers much higher quality and covers considerably wider variation\n","than existing high-resolution datasets (Appendix A). We have made this dataset publicly available, along with our source code and pre-trained networks.\n","The accompanying video can be found under the same link.\n","\n","Style-based generator\n","Traditionally the latent code is provided to the generator through an input layer, i.e.,\n","the first layer of a feedforward network (Figure 1a). We depart from this design by omitting\n","the input layer altogether and starting from a learned constant instead (Figure 1b, right).\n","Given a latent code z in the input latent space Z, a non-linear mapping network f : Z → W first produces w ∈ W\n","(Figure 1b, left). For simplicity, we set the dimensionality of both\n","https://github.com/NVlabs/stylegan\n","\"\"\""],"metadata":{"id":"Jnx-Auhg7-RH","executionInfo":{"status":"ok","timestamp":1688813315575,"user_tz":-120,"elapsed":378,"user":{"displayName":"Marcel Tracz","userId":"16306824067966008324"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["text2 = \"\"\"\n","A Sick Generator Architecture for Gen Z AI Networks\n","Tero Karras\n","NVIDIA\n","tkarras@nvidia.com\n","Samuli Laine\n","NVIDIA\n","slaine@nvidia.com\n","Timo Aila\n","NVIDIA\n","taila@nvidia.com\n","\n","Abstract\n","Yo, check out this alternative generator architecture for gen Z AI networks!\n","We borrowed some cool stuff from style transfer literature.\n","This fresh architecture automatically separates the high-level attributes\n","(like pose and identity for human faces) from the random variations (like freckles and hair) in the generated images.\n","Plus, it gives you precise control over the synthesis process.\n","It's dope because it beats the traditional quality metrics, produces smoother interpolation, and disentangles the hidden factors.\n","We even came up with two rad automated methods to measure interpolation quality and disentanglement for any generator architecture.\n","And guess what? We dropped a lit dataset of diverse human faces, called Flickr-Faces-HQ (FFHQ).\n","It's high-quality and way more diverse than existing datasets.\n","You can grab it, along with our source code and pre-trained networks. Peace!\n","\n","Introduction\n","Yo, the resolution and quality of images from gen Z AI networks (like GAN) have been on fleek lately,\n","thanks to rapid advancements [30, 45, 5]. But here's the thing: these generators are still black boxes,\n","and we don't fully understand how they work, especially when it comes to stochastic features.\n","We're clueless about the latent space too, and those fancy interpolations we see don't give us any real comparison between different generators.\n","So, inspired by style transfer literature, we decided to flip the script and redesign the generator architecture.\n","Our generator starts with a fixed input and tweaks the \"style\" at each convolution layer based on the latent code.\n","It's like controlling the strength of different image features at different scales.\n","We also added some noise to the mix, giving us that automatic separation of high-level attributes\n","(like pose and identity) from random variations (like freckles and hair) in the generated images.\n","And the best part? We didn't mess with the discriminator or the loss function.\n","Our work is a whole new level, separate from the GAN loss function discussions. It's straight fire!\n","Our generator embeds the input latent code into an intermediate space, and let us tell you,\n","it has a major impact on how the factors of variation are represented in the network.\n","The input latent space has to match the probability density of the training data, which inevitably leads to some entanglement.\n","But our intermediate latent space is different. It's free from that constraint and can be disentangled.\n","Since the existing methods for measuring the degree of latent space disentanglement didn't fit our vibe,\n","we came up with two new automated metrics—perceptual path length and linear separability—to measure that stuff.\n","And guess what? Our generator outshines the traditional architecture by delivering a more linear\n","and less tangled representation of the different factors of variation.\n","Finally, we drop the mic with a fresh dataset of human faces called Flickr-Faces-HQ (FFHQ).\n","It's top-notch, offering more quality and variety than any existing high-resolution datasets.\n","We made it public, along with our source code and pre-trained networks. Check out the video too.\n","It's all in the same place, fam!\n","\n","Style-based generator\n","Traditionally, the latent code gets input through the first layer of a feed-forward network (Figure 1a).\n","But we flipped it up and ditched that input layer completely. Instead, we start with a learned constant (Figure 1b, right).\n","We take a latent code z from the input latent space Z and run it through a non-linear mapping network f: Z → W,\n","which gives us w ∈ W (Figure 1b, left). To keep it simple, we set the dimensions of both spaces to be the same.\n","https://github.com/NVlabs/stylegan\n","\"\"\""],"metadata":{"id":"3CsiIlWh8BOX","executionInfo":{"status":"ok","timestamp":1688813318714,"user_tz":-120,"elapsed":2,"user":{"displayName":"Marcel Tracz","userId":"16306824067966008324"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["responses = list()\n","messages = prompt1 + '\\ninput: ' + text1 + '\\noutput: '\n","completion = model.predict(messages, **parameters)\n","\n","print(f\"Response from Model:\\n {completion.text}\")\n","\n","try:\n","  final_response = completion.text.split(delimiter)[-1].strip()\n","except Exception as e:\n","  final_response = \"Sorry, I'm having trouble right now, please try asking another question.\"\n","\n","print('\\n\\n', final_response)"],"metadata":{"id":"ox-avRChhKYI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688823221332,"user_tz":-120,"elapsed":3981,"user":{"displayName":"Marcel Tracz","userId":"16306824067966008324"}},"outputId":"3a2a60af-236c-497b-bf34-68204f254c6a"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Response from Model:\n"," #### Step 1:\n","The page has a coherent structure and logical arrangement of content. The introduction, results, discussion, and conclusions are appropriately presented.\n","#### Step 2:\n","The introduction is clear and concise, and it provides a good overview of the paper's topic. The results are presented in a clear and concise manner, and the discussion is well-organized and easy to follow. The conclusions are well-supported by the evidence presented in the paper.\n","#### Step 3:\n","There are no errors, gaps, or inconsistencies in the article. The substantive content and technical aspects are well-presented.\n","#### Step 4:\n","The conclusions align with the gathered data.\n","#### Step 5:\n","The author have correctly cited and referred to relevant sources.\n","#### Step 6:\n","The paper is written in a formal, objective, and precise language. The use of colloquialisms, slang expressions, and informal phrases is prohibited.\n","#### Step 7:\n","The formal review of the page is as follows:\n","The paper is well-written and easy to follow. The author presents a novel approach to the problem of generating high-quality images. The results are impressive, and the paper makes a significant contribution to the field.\n","\n","\n"," Step 7:\n","The formal review of the page is as follows:\n","The paper is well-written and easy to follow. The author presents a novel approach to the problem of generating high-quality images. The results are impressive, and the paper makes a significant contribution to the field.\n"]}]}]}